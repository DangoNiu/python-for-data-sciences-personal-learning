{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17745c53",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td width=15%><img src=\"./img/UGA.png\"></img></td>\n",
    "<td><center><h1>Introduction to Python for Data Sciences</h1></center></td>\n",
    "<td width=15%><a href=\"https://tung-qle.github.io/\" style=\"font-size: 16px; font-weight: bold\">Quoc-Tung Le</a> </td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e8017c",
   "metadata": {},
   "source": [
    "# 1 - Pytorch: a Numpy library that can differentiate\n",
    "\n",
    "Pytorch is a Python library that are arguably the most popular for deep learning. It contains many similar functions that are implemented in Numpy (see the [second notebook](2_Numpy_and_co.ipynb)). Attention: many functions of these two libraries might have the same names, but their functionalities can be (entirely) different!\n",
    "\n",
    "As we will see in this tutorial, in comparison to Numpy, Pytorch provides an additional important feature: Automatic Differentiation (AD, sometimes shortened to autodiff). That means if we implement a function using Pytorch library, we can compute its gradient with respect to its parameters efficiently. The gradient will be then used in the optimization algorithm (see Optimization course for more details).\n",
    "\n",
    "The following code demonstrates the autodiff feature of Pytorch: in the following, we want to compute the gradient of the function:\n",
    "\n",
    "$$f(x) = \\frac{1}{2}x^\\top \\mathbf{A} x,$$\n",
    "\n",
    "which are given by: $\\nabla f(x) = \\frac{1}{2}(\\mathbf{A} + \\mathbf{A}^\\top)x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56b8060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3365, -1.3077, -2.2670, -5.7812,  0.7818,  1.3851,  1.8301,  4.1606,\n",
      "        -1.6645, -2.7612, -0.6353,  0.2825, -1.7244,  3.4376,  1.7463,  2.1072,\n",
      "         1.8103, -2.4540, -4.4065, -1.6249])\n",
      "None\n",
      "Two vectors are equal. All is good\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "dim = 20\n",
    "# Create a parameter x and a matrix A\n",
    "x = torch.randn(dim, requires_grad=True)\n",
    "A = torch.randn((dim, dim))\n",
    "\n",
    "# Compute the function f(x) and assign to the variable y\n",
    "y = 0.5 * torch.dot(x, torch.matmul(A, x))\n",
    "\n",
    "# Differentiating the function f by calling y.backward()\n",
    "y.backward()\n",
    "\n",
    "# Accessing the gradient of f with respect to x\n",
    "print(x.grad)\n",
    "print(A.grad)\n",
    "\n",
    "# Checking the calculation with the closed form gradient formula\n",
    "try:\n",
    "    torch.testing.assert_close(0.5 * torch.matmul(A + A.T,x), x.grad)\n",
    "    print(\"Two vectors are equal. All is good\")\n",
    "except:\n",
    "    print(\"Wrong calculation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8571b8bf",
   "metadata": {},
   "source": [
    "## 1.1 - How to create Pytorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdeb76a",
   "metadata": {},
   "source": [
    "There are many different methods to create a Pytorch tensor, either using Python list, numpy array or even randomization. They are shown in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba1fdb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [-3., -2., -1.]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize a tensor using Python list\n",
    "x = torch.tensor([[1.0, 2.0, 3.0], [-3.0, -2.0, -1.0]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9b8c8",
   "metadata": {},
   "source": [
    "Unlike a Numpy array, a Pytorch tensor has many metadata fields. The following code shows how to access the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad38f66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor info:\n",
      "  shape        : (2, 3)\n",
      "  size         : torch.Size([2, 3])\n",
      "  dtype        : torch.float32\n",
      "  device       : cpu\n",
      "  requires_grad: False\n",
      "  is_leaf      : True\n",
      "  data         :\n",
      "tensor([[ 1.,  2.,  3.],\n",
      "        [-3., -2., -1.]])\n"
     ]
    }
   ],
   "source": [
    "def show_info(x):\n",
    "    print(\"Tensor info:\")\n",
    "    print(f\"  shape        : {tuple(x.shape)}\")\n",
    "    print(f\"  size         : {x.size()}\")\n",
    "    print(f\"  dtype        : {x.dtype}\")\n",
    "    print(f\"  device       : {x.device}\")         # The output is either CPU or GPU, depending on the your implementation and hardware\n",
    "    print(f\"  requires_grad: {x.requires_grad}\")  # If requires_grad = False, it is impossible to differentiate a function w.r.t. x. See the example with the quadratic function\n",
    "    print(f\"  is_leaf      : {x.is_leaf}\")        # is_leaf = True if and only if requires_grad = False (convention) or it is initialized by the user and not a result of some operations\n",
    "    print(f\"  data         :\\n{x}\")    \n",
    "\n",
    "show_info(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493eb2df",
   "metadata": {},
   "source": [
    "One can manually assign specific values for these metadata fields right at the moment or after creation.\n",
    "\n",
    "Pay attention to the field _requires\\_grad_ : it determines whether a variable can be differentiated or not. Therefore, when using Pytorch to perform optimization tasks, you need to ensure that _requires\\_grad_ is __True__. Otherwise, the parameter will never be updated (we will see more about this in the exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4880c91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor info:\n",
      "  shape        : (2, 3)\n",
      "  size         : torch.Size([2, 3])\n",
      "  dtype        : torch.float16\n",
      "  device       : cpu\n",
      "  requires_grad: True\n",
      "  is_leaf      : False\n",
      "  data         :\n",
      "tensor([[ 1.,  2.,  3.],\n",
      "        [-3., -2., -1.]], dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "Tensor info:\n",
      "  shape        : (2, 3)\n",
      "  size         : torch.Size([2, 3])\n",
      "  dtype        : torch.float16\n",
      "  device       : cpu\n",
      "  requires_grad: True\n",
      "  is_leaf      : True\n",
      "  data         :\n",
      "tensor([[ 1.,  2.,  3.],\n",
      "        [-3., -2., -1.]], dtype=torch.float16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# We modify several metadata field of the same tensor x that we created previously\n",
    "\n",
    "x.requires_grad = True\n",
    "x = x.to(dtype = torch.float16)\n",
    "\n",
    "show_info(x)\n",
    "# We can also create a new tensor x, with desired metadata \n",
    "y = torch.tensor([[1, 2, 3], [-3, -2, -1]], requires_grad = True, dtype = torch.float16)\n",
    "show_info(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b820b",
   "metadata": {},
   "source": [
    "One can use a Numpy array as the value of a newly-created Pytorch tensor as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92de12c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4]\n",
      " [ 5  6  7  8  9]\n",
      " [10 11 12 13 14]\n",
      " [15 16 17 18 19]]\n",
      "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14.],\n",
      "        [15., 16., 17., 18., 19.]])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor using Numpy arrays\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_array = np.reshape(np.arange(20), (4,5))\n",
    "x = torch.Tensor(x_array)\n",
    "print(x_array)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff205675",
   "metadata": {},
   "source": [
    "Finally, Pytorch offers several methods of creating random or special tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b51496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9572, 0.8288],\n",
      "        [0.7195, 0.2063],\n",
      "        [0.6423, 0.1076]])\n",
      "torch.Size([3, 2])\n",
      "tensor([[-0.3887, -1.5541],\n",
      "        [-0.4626,  0.3839],\n",
      "        [ 1.1924, -0.3491]])\n",
      "torch.Size([3, 2])\n",
      "All ones matrix:\n",
      " tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "All zeros matrix:\n",
      " tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "Identity matrix:\n",
      " tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor by randomization\n",
    "\n",
    "# Using uniform distribution\n",
    "x = torch.rand((3,2))\n",
    "print(x)\n",
    "print(x.size())\n",
    "\n",
    "# Using Gaussian distribution\n",
    "x = torch.randn((3,2))\n",
    "print(x)\n",
    "print(x.size())\n",
    "\n",
    "# Creat an identity, all-one and all-zero matrices\n",
    "x_all_ones = torch.ones((3,2))\n",
    "x_all_zeros = torch.zeros((3,2))\n",
    "x_identity = torch.eye(3)  \n",
    "\n",
    "print(\"All ones matrix:\\n {}\".format(x_all_ones))\n",
    "print(\"All zeros matrix:\\n {}\".format(x_all_zeros))\n",
    "print(\"Identity matrix:\\n {}\".format(x_identity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee3abe1",
   "metadata": {},
   "source": [
    "## 1.2 - Operation on Pytorch tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcd2b66",
   "metadata": {},
   "source": [
    "Pytorch provides a plethora of tensor operations. For a complete presentation of its functionalities, you are advised to visit [this Pytorch documentation](https://docs.pytorch.org/docs/stable/torch.html). In the following, we will only introduce several frequent operations.\n",
    "\n",
    "### 1.2.1 - Slicing, Joining and Mutating Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31d18c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      " tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
      "        72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
      "        90, 91, 92, 93, 94, 95, 96, 97, 98, 99]) \n",
      "\n",
      "Reshape to a matrix 10 x 10:\n",
      " tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
      "        [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
      "        [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
      "        [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
      "        [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
      "        [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n",
      "        [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],\n",
      "        [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],\n",
      "        [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]])\n",
      "\n",
      "Reshape to a tensor of dimensions 2 x 5 x 5 x 2: \n",
      " tensor([[[[ 0,  1],\n",
      "          [ 2,  3],\n",
      "          [ 4,  5],\n",
      "          [ 6,  7],\n",
      "          [ 8,  9]],\n",
      "\n",
      "         [[10, 11],\n",
      "          [12, 13],\n",
      "          [14, 15],\n",
      "          [16, 17],\n",
      "          [18, 19]],\n",
      "\n",
      "         [[20, 21],\n",
      "          [22, 23],\n",
      "          [24, 25],\n",
      "          [26, 27],\n",
      "          [28, 29]],\n",
      "\n",
      "         [[30, 31],\n",
      "          [32, 33],\n",
      "          [34, 35],\n",
      "          [36, 37],\n",
      "          [38, 39]],\n",
      "\n",
      "         [[40, 41],\n",
      "          [42, 43],\n",
      "          [44, 45],\n",
      "          [46, 47],\n",
      "          [48, 49]]],\n",
      "\n",
      "\n",
      "        [[[50, 51],\n",
      "          [52, 53],\n",
      "          [54, 55],\n",
      "          [56, 57],\n",
      "          [58, 59]],\n",
      "\n",
      "         [[60, 61],\n",
      "          [62, 63],\n",
      "          [64, 65],\n",
      "          [66, 67],\n",
      "          [68, 69]],\n",
      "\n",
      "         [[70, 71],\n",
      "          [72, 73],\n",
      "          [74, 75],\n",
      "          [76, 77],\n",
      "          [78, 79]],\n",
      "\n",
      "         [[80, 81],\n",
      "          [82, 83],\n",
      "          [84, 85],\n",
      "          [86, 87],\n",
      "          [88, 89]],\n",
      "\n",
      "         [[90, 91],\n",
      "          [92, 93],\n",
      "          [94, 95],\n",
      "          [96, 97],\n",
      "          [98, 99]]]])\n",
      "Same results\n"
     ]
    }
   ],
   "source": [
    "# Torch reshape\n",
    "x = torch.tensor([i for i in range(100)])\n",
    "print(\"Original tensor:\\n {} \\n\".format(x))\n",
    "\n",
    "# Reshape x to (10,10)\n",
    "x_reshaped_1 = torch.reshape(x, (10, 10))\n",
    "print(\"Reshape to a matrix 10 x 10:\\n {}\\n\".format(x_reshaped_1))\n",
    "\n",
    "# Reshape x to (2, 5, 5, 2)\n",
    "x_reshaped_2 = torch.reshape(x, (2, 5, 5, 2))\n",
    "print(\"Reshape to a tensor of dimensions 2 x 5 x 5 x 2: \\n {}\".format(x_reshaped_2))\n",
    "\n",
    "# You can also obtain the same result using x_reshape_1\n",
    "\n",
    "try:\n",
    "    torch.testing.assert_close(x_reshaped_2, torch.reshape(x_reshaped_1, (2,5,5,2)))\n",
    "    print(\"Same results\")\n",
    "except:\n",
    "    print(\"Different results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8da23494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "First method:\n",
      " tensor([[0, 5],\n",
      "        [1, 6],\n",
      "        [2, 7],\n",
      "        [3, 8],\n",
      "        [4, 9]])\n",
      "Second method:\n",
      " tensor([[0, 5],\n",
      "        [1, 6],\n",
      "        [2, 7],\n",
      "        [3, 8],\n",
      "        [4, 9]])\n",
      "torch.Size([3, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# Torch dimension transpose and permutation \n",
    "x = torch.arange(10).reshape((2,5))\n",
    "print(x)\n",
    "\n",
    "# For 2D tensor, transpose can be simply done by\n",
    "x_transpose_1 = x.T\n",
    "print(\"First method:\\n {}\".format(x_transpose_1))\n",
    "\n",
    "# For general tensor with multiple dimension, use torch.transpose(your_tensor, dim0, dim1)\n",
    "# dim0, dim1: two dimensions that you want to switch\n",
    "x_transpose_2 = torch.transpose(x, 0, 1)\n",
    "print(\"Second method:\\n {}\".format(x_transpose_2))\n",
    "\n",
    "x_3d_tensor = torch.arange(30).reshape((2,3,5))\n",
    "x_transpose_3 = torch.permute(x_3d_tensor, (1,2,0))\n",
    "print(x_transpose_3.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2bef247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the first new concatenated tensor: torch.Size([8, 5])\n",
      "Size of the second new concatenated tensor: torch.Size([5, 8])\n"
     ]
    }
   ],
   "source": [
    "# Torch concatenate\n",
    "# Concatenate a matrix 2 x 5 with another of size 6 x 5 to get a 8 x 5 matrix\n",
    "x = torch.arange(10).reshape((2,5))\n",
    "y = torch.arange(30).reshape((6,5))\n",
    "z = torch.concat([x,y], dim = 0)\n",
    "print(\"Size of the first new concatenated tensor: {}\".format(z.size()))\n",
    "\n",
    "# Concatenate a matrix 5 x 2 with another of size 5 x 6 to get a 5 x 8 matrix\n",
    "x = torch.arange(10).reshape((5,2))\n",
    "y = torch.arange(30).reshape((5,6))\n",
    "z = torch.concat([x,y], dim = 1)\n",
    "print(\"Size of the second new concatenated tensor: {}\".format(z.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b3c80e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 2])\n",
      "torch.Size([3, 5, 2])\n",
      "torch.Size([3, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "# Torch stack\n",
    "# In comparison to torch concatenate, torch stack creates a new tensor with one more dimension\n",
    "\n",
    "tensor_list = [torch.randn(3,2) for i in range(5)]\n",
    "tensor_stacked_0 = torch.stack(tensor_list, dim = 0)\n",
    "tensor_stacked_1 = torch.stack(tensor_list, dim = 1)\n",
    "tensor_stacked_2 = torch.stack(tensor_list, dim = 2)\n",
    "\n",
    "print(tensor_stacked_0.size())\n",
    "print(tensor_stacked_1.size())\n",
    "print(tensor_stacked_2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698d08b",
   "metadata": {},
   "source": [
    "### 1.2.2 - Pointwise Operations\n",
    "\n",
    "Similar to Numpy, Pytorch has many different function for pointwise operations, i.e., those of the forms:\n",
    "\n",
    "$$\\begin{pmatrix} x_{i_1\\ldots i_n} \\end{pmatrix}_{i_1,\\ldots,i_n} \\mapsto \\begin{pmatrix} f(x_{i_1\\ldots i_n}) \\end{pmatrix}_{i_1,\\ldots,i_n} \\qquad \\text{or} \\qquad \\begin{pmatrix} x_{i_1\\ldots i_n} \\end{pmatrix}_{i_1,\\ldots,i_n} \\times \\begin{pmatrix} y_{i_1\\ldots i_n} \\end{pmatrix}_{i_1,\\ldots,i_n} \\mapsto \\begin{pmatrix} g(x_{i_1\\ldots i_n}, y_{i_1\\ldots i_n}) \\end{pmatrix}_{i_1,\\ldots,i_n}$$\n",
    "\n",
    "where $f: \\mathbb{R} \\to \\mathbb{R}$ and $g: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39f8fb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9236,  0.1250, -0.3960, -0.0285,  2.2730],\n",
      "        [-0.6047, -0.1278,  1.1359,  0.6149,  2.2092],\n",
      "        [ 0.5945, -0.0332, -3.0123, -1.6577, -1.3400]])\n",
      "tensor([[ 0.9384,  0.1247, -0.3857, -0.0285,  0.7634],\n",
      "        [-0.5685, -0.1274,  0.9069,  0.5769,  0.8031],\n",
      "        [ 0.5601, -0.0332, -0.1290, -0.9962, -0.9735]])\n",
      "tensor([[-0.3455,  0.9922,  0.9226,  0.9996, -0.6459],\n",
      "        [ 0.8227,  0.9919,  0.4213,  0.8168, -0.5959],\n",
      "        [ 0.8284,  0.9994, -0.9916, -0.0868,  0.2288]])\n",
      "tensor([[1.9236, 0.1250, 0.0000, 0.0000, 2.2730],\n",
      "        [0.0000, 0.0000, 1.1359, 0.6149, 2.2092],\n",
      "        [0.5945, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[1.9236, 0.1250, 0.3960, 0.0285, 2.2730],\n",
      "        [0.6047, 0.1278, 1.1359, 0.6149, 2.2092],\n",
      "        [0.5945, 0.0332, 3.0123, 1.6577, 1.3400]])\n",
      "tensor([[6.8454, 1.1331, 0.6730, 0.9719, 9.7089],\n",
      "        [0.5462, 0.8801, 3.1140, 1.8494, 9.1084],\n",
      "        [1.8122, 0.9673, 0.0492, 0.1906, 0.2618]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,5)\n",
    "\n",
    "# f(x) = sin(x), cos(x), ReLU(x), |x|, exponent\n",
    "\n",
    "print(x)\n",
    "print(torch.sin(x))\n",
    "print(torch.cos(x))\n",
    "print(torch.relu(x))\n",
    "print(torch.abs(x))\n",
    "print(torch.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8982779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original value:\n",
      " tensor([[-0.9731,  0.0520, -0.2603, -0.2928, -0.0179],\n",
      "        [-0.7279,  2.0282,  1.4706, -2.1094, -0.2772],\n",
      "        [ 1.1572, -1.3377,  0.8996, -0.4114,  0.2723]])\n",
      "New value:\n",
      " tensor([[-0.8266,  0.0520, -0.2574, -0.2886, -0.0179],\n",
      "        [-0.6653,  0.8972,  0.9950, -0.8584, -0.2737],\n",
      "        [ 0.9157, -0.9730,  0.7831, -0.3999,  0.2689]])\n"
     ]
    }
   ],
   "source": [
    "# Note that these previous functions do not change the value of x. In fact, they will compute the value f(x) and save it to a new memory allocation\n",
    "# We can compute the function in-place by adding _ at the end of these functions\n",
    "\n",
    "x = torch.randn(3,5)\n",
    "print(\"Original value:\\n {}\".format(x))\n",
    "torch.sin_(x)\n",
    "print(\"New value:\\n {}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff147d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4831,  0.3268, -1.6203,  1.3995, -2.1340],\n",
      "        [-1.0748,  0.0662, -0.9080,  2.0779, -0.5779],\n",
      "        [ 1.6677,  0.6566, -0.5541, -2.9831, -2.0007]])\n",
      "tensor([[-1.7067, -0.3064, -0.7179, -0.6240,  2.3520],\n",
      "        [-0.8898, -1.8300,  0.3410,  0.9940,  2.8268],\n",
      "        [ 0.1448,  0.9472, -2.1386, -3.2259,  0.6446]])\n",
      "tensor([[-0.6698,  0.0032,  0.5275,  0.3923, -0.2445],\n",
      "        [ 0.0909, -0.8362,  0.1771,  0.8325, -1.9142],\n",
      "        [ 0.6901, -0.1165, -1.0666, -0.3769,  0.8968]])\n",
      "tensor([[ -0.5587,   0.0322,   2.5910,   0.3833,  -0.0486],\n",
      "        [ 10.6206,  -0.9302,   0.4540,   2.8340,  -0.6605],\n",
      "        [  1.1902,  -5.5197,  -1.6994, -25.5746,   0.5127]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,5)\n",
    "y = torch.randn(3,5)\n",
    "\n",
    "# g(x, y) = x + y, x - y, x * y, x / y\n",
    "print(x + y)\n",
    "print(x - y)\n",
    "print(x * y)\n",
    "print(x / y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6176973",
   "metadata": {},
   "source": [
    "In the previous example, noticing that $x$ and $y$ share the same shapes. However, it is also possible to perform operations for $x$ and $y$ of different shapes under certain conditions. Broadcasting allows to reduce memory footprint, simplify code and optimized performance.\n",
    "\n",
    "Consider $x \\in \\mathbb{R}^{3 \\times 5}$ and $y \\in \\mathbb{R}^{5}$. The value of $x + y$ will be given by $x + y^\\star$ where $y^\\star \\in \\mathbb{R}^{3 \\times 5}$ is a matrix whose rows are all equal to $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "315bef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1587, -2.3399, -0.7537,  1.6816, -2.1473],\n",
      "        [-0.3771, -0.0312,  1.3968,  0.2348, -0.7198],\n",
      "        [ 1.9391, -0.1018, -0.6971, -1.6533, -0.5395]])\n",
      "tensor([[ 1.1587, -2.3399, -0.7537,  1.6816, -2.1473],\n",
      "        [-0.3771, -0.0312,  1.3968,  0.2348, -0.7198],\n",
      "        [ 1.9391, -0.1018, -0.6971, -1.6533, -0.5395]])\n",
      "tensor([[ 1.8817, -1.4485, -0.4668,  1.7063, -0.5627],\n",
      "        [-0.9677, -0.4534,  0.3701, -1.0541, -0.4488],\n",
      "        [ 1.9010,  0.0284, -1.1714, -2.3898,  0.2839]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,5)\n",
    "y = torch.randn(5)\n",
    "\n",
    "# Use broadcasting\n",
    "print(x + y)\n",
    "\n",
    "# More explicit code, but lengthy and not memory efficient because we need to explicitly compute y*\n",
    "print(x + torch.stack([y for i in range(3)], dim = 0))\n",
    "\n",
    "# If you want to add a certain vector to all columns, then do the following\n",
    "z = torch.randn(3)\n",
    "\n",
    "# First, reshape the vector to (3,1). Pytorch broadcasting will handle the rest\n",
    "z = z.reshape((3,1))\n",
    "print(x + z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa335a1",
   "metadata": {},
   "source": [
    "### 1.2.3 - Other useful operations\n",
    "\n",
    "Dot product between two tensors - $\\mathtt{torch.dot}$ and tensor norm - $\\mathtt{torch.norm}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efdb355c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product between two tensors: -1.6195298433303833\n",
      "Frobenius norm: 10.435500144958496\n",
      "Operator norm: 5.635004997253418\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "y = torch.randn(3)\n",
    "\n",
    "# torch.dot(x, y) \n",
    "dot_product = torch.dot(x,y)\n",
    "print(\"Dot product between two tensors: {}\".format(dot_product))\n",
    "\n",
    "# torch.norm\n",
    "# L2 norm (or the Euclidean distance to 0) of a vector\n",
    "norm_l2 = torch.linalg.norm(x)\n",
    "\n",
    "# For matrix, additional parameters can be specified so that we can compute different matrix norm\n",
    "x = torch.randn(10, 9)\n",
    "\n",
    "# Frobenius norm\n",
    "fro_norm = torch.linalg.norm(x)\n",
    "print(\"Frobenius norm: {}\".format(fro_norm))\n",
    "\n",
    "# Operator norm: op(A) = \\max {\\|Ax\\|_2 | \\|x\\|_2 = 1}\n",
    "op_norm = torch.linalg.norm(x, ord = 2)\n",
    "print(\"Operator norm: {}\".format(op_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ed07a",
   "metadata": {},
   "source": [
    "Matrix-matrix and matrix-vector multiplicaiton - $\\mathtt{torch.matmul}$ and its batch version - $\\mathtt{torch.bmm}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73648d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5102, -5.0278,  2.4971,  0.6991],\n",
      "        [-1.3725, -2.2848,  1.8377, -0.3113],\n",
      "        [-0.0607,  2.3569, -6.4740, -2.7508]])\n",
      "tensor([[ 0.5102, -5.0278,  2.4971,  0.6991],\n",
      "        [-1.3725, -2.2848,  1.8377, -0.3113],\n",
      "        [-0.0607,  2.3569, -6.4740, -2.7508]])\n",
      "tensor([-1.9217, -0.3279, -0.9894])\n",
      "torch.Size([10, 3, 4])\n",
      "Using loop takes 0.2160515310001756 seconds\n",
      "Using bmm takes 0.059521857000163436 seconds\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,5)\n",
    "y = torch.randn(5,4)\n",
    "\n",
    "# Compute the product using torch.matmul\n",
    "print(torch.matmul(x,y))\n",
    "\n",
    "# We can use @ instead\n",
    "print(x @ y)\n",
    "\n",
    "# torch.matmul can be used also to compute the product between a matrix and a vector (and we get a vector as the result)\n",
    "z = torch.randn(5)\n",
    "print(torch.matmul(x,z))\n",
    "\n",
    "# torch.bmm can be used to compute matrix multiplication by batch\n",
    "\n",
    "x = torch.randn(10,3,5) # A batch of matrices (a 3D tensor of size batch_size x dim_1 x dim_2)\n",
    "y = torch.randn(10,5,4) # A batch of matrices (a 3D tensor of size batch_size x dim_2 x dim_3)\n",
    "z = torch.bmm(x,y) # A batch of matrices (of size batch_size x dim_1 x dim_3)\n",
    "print(z.size())\n",
    "\n",
    "# One can compare the performance between torch.bmm and torch.mm when it comes to large batches of matrix multiplication\n",
    "# In general, the larger is the batch size, the more efficient is torch.bmm in comparison to torch.mm \n",
    "\n",
    "import time\n",
    "\n",
    "dim = 100\n",
    "batch_size = 5000\n",
    "x = torch.randn(batch_size, dim, dim)\n",
    "y = torch.randn(batch_size, dim, dim)\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "z = torch.zeros(batch_size, dim, dim)\n",
    "for idx in range(len(x)):\n",
    "    z[idx,:,:] = torch.matmul(x[idx,:,:], y[idx, :, :])\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(\"Using loop takes {} seconds\".format(end - start))\n",
    "\n",
    "start = time.perf_counter()\n",
    "z = torch.bmm(x,y)\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(\"Using bmm takes {} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a6502",
   "metadata": {},
   "source": [
    "Matrix inversion - $\\mathtt{torch.inverse}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b4d5583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2798,  0.0622, -0.6481],\n",
      "        [-0.3564,  0.1278,  0.4375],\n",
      "        [-0.0744,  0.7727,  0.1693]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,3)\n",
    "print(torch.inverse(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f5b1c",
   "metadata": {},
   "source": [
    "Singular Value Decomposition: Given a matrix $\\mathbf{A}$ (of arbitrary dimension), it can always be written as:\n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{U}\\mathbf{S}\\mathbf{V}^\\top$$\n",
    "\n",
    "where $\\mathbf{U}, \\mathbf{V}$ are orthogonal and $\\mathbf{S}$ is a (possibly rectangular) diagonal matrix. Pytorch allows to compute such decomposition with $\\mathtt{torch.linalg.svd}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d95cb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8889,  0.1393, -0.4365],\n",
      "        [-0.4009,  0.6977, -0.5937],\n",
      "        [-0.2218, -0.7027, -0.6760]])\n",
      "tensor([1.5274, 1.0109, 0.2802])\n",
      "tensor([[ 0.9686,  0.1166, -0.2186,  0.0198],\n",
      "        [-0.0577,  0.5815, -0.0191, -0.8112],\n",
      "        [-0.2405,  0.4011, -0.8224,  0.3240],\n",
      "        [ 0.0245,  0.6981,  0.5249,  0.4863]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,4)\n",
    "u, s, v = torch.linalg.svd(x)\n",
    "print(u)\n",
    "print(s)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582bf51c",
   "metadata": {},
   "source": [
    "## 1.3 - Compute the gradient of a function\n",
    "\n",
    "Finally, we investiage how to compute the gradient of a function $f$ w.r.t. a variable $x$. For a concrete example, we consider the following function:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(\\mathbf{U}, \\mathbf{V}) = \\|\\mathbf{A} - \\mathbf{U}\\mathbf{V}^\\top\\|_F^2 \\qquad \\text{where} \\qquad \\mathbf{U} \\in \\mathbb{R}^{m \\times r}, \\mathbf{V} \\in \\mathbb{R}^{n \\times r}, \\mathbf{A} \\in \\mathbb{R}^{m \\times n}\n",
    "\\end{equation}\n",
    "\n",
    "We will compute the gradient w.r.t. $\\mathbf{U}$ and $\\mathbf{V}$ using the following code where $m = n = 10, r = 2$ (you can change these parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "df912f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "n = 10\n",
    "r = 2\n",
    "\n",
    "# Initialize u and v (using any method in the section 1.1)\n",
    "A = torch.randn(m,n)\n",
    "\n",
    "# Do not forget to set requires_grad = True for U and V (the variables that we want to differentiate w.r.t.)\n",
    "U = torch.rand(m, r, requires_grad = True)\n",
    "V = torch.rand(n, r, requires_grad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2994b949",
   "metadata": {},
   "source": [
    "From here, there are two main Pytorch functions that can be used for the automatic differentiation: $\\mathtt{torch.grad.autograd()}$ and $\\mathtt{backward()}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b6e2cf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of U:\n",
      " tensor([[ 7.9559,  6.0539],\n",
      "        [ 4.1504,  7.5828],\n",
      "        [ 2.5371,  2.4449],\n",
      "        [ 1.8039,  3.5000],\n",
      "        [ 6.1956,  4.1425],\n",
      "        [11.0393, 15.0167],\n",
      "        [ 6.4536,  4.7267],\n",
      "        [ 6.1653,  7.8245],\n",
      "        [ 2.6021,  0.4974],\n",
      "        [ 3.1821,  5.8985]])\n",
      "Gradient of V:\n",
      " tensor([[ 7.5575,  4.3580],\n",
      "        [ 6.3633,  7.8859],\n",
      "        [ 0.4309,  1.0458],\n",
      "        [-4.6868, -5.2338],\n",
      "        [ 6.7658,  7.0737],\n",
      "        [ 4.1258,  6.5767],\n",
      "        [-0.4851,  7.4583],\n",
      "        [ 2.0126,  5.9917],\n",
      "        [10.3318, -2.1440],\n",
      "        [ 9.9368,  7.7034]])\n",
      "Gradient of U after two backwards:\n",
      " tensor([[15.9118, 12.1078],\n",
      "        [ 8.3009, 15.1656],\n",
      "        [ 5.0741,  4.8898],\n",
      "        [ 3.6078,  7.0000],\n",
      "        [12.3912,  8.2849],\n",
      "        [22.0786, 30.0333],\n",
      "        [12.9071,  9.4533],\n",
      "        [12.3307, 15.6491],\n",
      "        [ 5.2042,  0.9948],\n",
      "        [ 6.3641, 11.7971]])\n",
      "Gradient of V after two backwards:\n",
      " tensor([[ 15.1151,   8.7161],\n",
      "        [ 12.7265,  15.7719],\n",
      "        [  0.8618,   2.0916],\n",
      "        [ -9.3736, -10.4676],\n",
      "        [ 13.5315,  14.1475],\n",
      "        [  8.2515,  13.1534],\n",
      "        [ -0.9702,  14.9165],\n",
      "        [  4.0252,  11.9833],\n",
      "        [ 20.6635,  -4.2880],\n",
      "        [ 19.8735,  15.4068]])\n"
     ]
    }
   ],
   "source": [
    "# Using backward()\n",
    "\n",
    "loss_fn = torch.linalg.norm(A - U @ V.T) ** 2\n",
    "loss_fn.backward()\n",
    "\n",
    "# Acessing the gradient of U and V using their field .grad\n",
    "\n",
    "print(\"Gradient of U:\\n {}\".format(U.grad))\n",
    "print(\"Gradient of V:\\n {}\".format(V.grad))\n",
    "\n",
    "# Pay attention, if you backward twice, the gradient will be doubled\n",
    "# In fact, the field .grad is accumulated: what you get is the sum of gradients of all previous backward() calls\n",
    "loss_fn = torch.linalg.norm(A - U @ V.T) ** 2\n",
    "loss_fn.backward()\n",
    "\n",
    "# Acessing the gradient of U and V using their field .grad\n",
    "\n",
    "print(\"Gradient of U after two backwards:\\n {}\".format(U.grad))\n",
    "print(\"Gradient of V after two backwards:\\n {}\".format(V.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf2319d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of U:\n",
      " tensor([[ 7.8927,  3.0555],\n",
      "        [ 3.4071,  0.8600],\n",
      "        [10.9252,  8.3503],\n",
      "        [ 9.4930,  2.0924],\n",
      "        [12.5562,  8.7307],\n",
      "        [ 4.9408,  3.9615],\n",
      "        [ 6.5477, 12.6190],\n",
      "        [ 7.6172,  6.3270],\n",
      "        [12.2745, 12.2227],\n",
      "        [ 8.4445,  6.1313]])\n",
      "Gradient of V:\n",
      " tensor([[ 5.3973, 11.1532],\n",
      "        [ 1.4212, -0.7133],\n",
      "        [ 6.3601,  3.1715],\n",
      "        [ 8.4189,  8.9069],\n",
      "        [ 1.6081, 10.8099],\n",
      "        [ 4.6290,  7.5384],\n",
      "        [13.4344, 11.1798],\n",
      "        [12.5221, 11.0223],\n",
      "        [ 2.2346,  4.4744],\n",
      "        [ 3.7380,  5.8154]])\n"
     ]
    }
   ],
   "source": [
    "# Using torch.grad.autograd(): you need to write a Python function that takes U and V as parameters and return f as the result\n",
    "\n",
    "loss_fn = torch.linalg.norm(A - U @ V.T) ** 2\n",
    "\n",
    "gradients = torch.autograd.grad(loss_fn, inputs = [U, V])\n",
    "\n",
    "# torch.autograd.grad returns a list of gradients whose order is the same as the list of parameters that we provide for the argument inputs\n",
    "\n",
    "print(\"Gradient of U:\\n {}\".format(gradients[0]))\n",
    "print(\"Gradient of V:\\n {}\".format(gradients[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f51ef438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will get None because the field A.requires_grad = False\n",
      "Now we get the gradient of A w.r.t. f: \n",
      " tensor([[-1.4920,  1.1833, -0.3971, -0.5585,  3.0697,  0.2429,  0.4414, -4.4077,\n",
      "         -3.6110,  0.7437],\n",
      "        [-0.3568, -3.3341, -2.0782, -0.2510, -3.0174, -1.5164, -2.0860,  0.3596,\n",
      "         -0.9220,  0.8905],\n",
      "        [-4.0037, -1.5907, -0.5586, -3.2769, -0.0642,  1.4206, -3.8088, -2.4067,\n",
      "         -0.0422,  0.3524],\n",
      "        [-1.3092, -2.2984, -3.4772, -0.8878,  2.3446,  0.1379, -3.8952, -5.0607,\n",
      "         -0.9229,  1.3610],\n",
      "        [-2.1617, -0.7055, -3.9768, -3.9335, -3.5363, -5.3020, -1.0753, -1.8726,\n",
      "         -1.0962, -1.3823],\n",
      "        [ 0.9136,  1.4145, -1.8772, -1.2072, -1.0295, -1.2667, -1.2855, -4.2258,\n",
      "         -0.8246,  0.1640],\n",
      "        [-1.0351,  0.5156,  0.4445,  0.5564,  0.6262, -0.4407, -1.1818, -3.7739,\n",
      "          2.7466, -3.2798],\n",
      "        [-2.0172,  1.1386,  1.5795, -4.0885,  2.9253,  1.0934, -1.9789,  0.0205,\n",
      "          0.4236, -2.0339],\n",
      "        [-2.5574,  0.8024, -5.3165, -6.5069, -0.5562, -2.4405, -2.0069, -1.8660,\n",
      "         -0.7218, -1.5850],\n",
      "        [-2.9433,  0.4649, -3.2656,  2.9015, -1.6917,  1.1575, -0.8091,  1.6172,\n",
      "          0.7487, -1.8801]])\n"
     ]
    }
   ],
   "source": [
    "# Differentiate w.r.t. the matrix A\n",
    "A = torch.randn(m,n)\n",
    "\n",
    "loss_fn = torch.linalg.norm(A - U @ V.T) ** 2\n",
    "loss_fn.backward()\n",
    "\n",
    "print(f\"We will get {A.grad} because the field A.requires_grad = {A.requires_grad}\")\n",
    "\n",
    "# Now change it\n",
    "A.requires_grad = True\n",
    "\n",
    "# Remember to re-define loss_fn, otherwise, you cannot differentiate anymore\n",
    "loss_fn = torch.linalg.norm(A - U @ V.T) ** 2\n",
    "loss_fn.backward()\n",
    "\n",
    "print(\"Now we get the gradient of A w.r.t. f: \\n {}\".format(A.grad))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd9602",
   "metadata": {},
   "source": [
    "# 2 - Several Pytorch modules for neural networks training\n",
    "\n",
    "Pytorch provides users with many modules and built-in functions that are useful in both academic and industrial research and deployment of deep learning. Since our course provides only an overview of the main functionalities, we only deal with the vanilla deep learning architectures: the Multi-Layer Perceptrons (MLP)s and Pytorch components related to these architectures.\n",
    "\n",
    "Given $\\theta = \\{(\\mathbf{W_\\ell}, b_\\ell) \\mid \\ell = 1, \\ldots, L\\}$ a sequence of pairs of matrices and biases, the corresponding MLP encodes the following function:\n",
    "\n",
    "\\begin{equation}\n",
    "    f_\\theta (x) = \\mathbf{W}_L \\sigma (\\mathbf{W}_{L-1} \\sigma (\\ldots \\sigma(\\mathbf{W}_1 x + b_1) \\ldots ) + b_{L-1}) + b_L\n",
    "\\end{equation}\n",
    "\n",
    "or equivalently, \n",
    "\\begin{equation}\n",
    "    f_\\theta (x) = f_{\\mathbf{W}_L, b_L} \\circ \\sigma \\circ \\ldots \\sigma \\circ f_{\\mathbf{W_1}, b_1} (x)\n",
    "\\end{equation}\n",
    "\n",
    "where $f_{\\mathbf{W}_\\ell, b_\\ell}(x) = \\mathbf{W}_\\ell x + b_\\ell$ is an affine function and $\\sigma: \\mathbb{R}^d \\to \\mathbb{R}^d$ is a pointwise function such as $\\mathtt{torch.ReLU}$ or $\\mathtt{torch.exp}, \\ldots$ (see Section 1.2.2). You can use the built-in Pytorch functions for these components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "64ec637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_params(model: torch.nn.Module):\n",
    "    print(f\"Model: {model.__class__.__name__}\\n\")\n",
    "    \n",
    "    print(f\"{'Parameter':25} {'Shape':25} {'Requires_grad'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name:25} {str(list(param.shape)):25} {param.requires_grad}\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "741264f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Linear\n",
      "\n",
      "Parameter                 Shape                     Requires_grad\n",
      "----------------------------------------------------------------------\n",
      "weight                    [15, 10]                  True\n",
      "bias                      [15]                      True\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Total parameters: 165\n",
      "tensor([-0.8236, -0.7631,  0.6164,  0.6084,  0.3192,  0.1023,  0.8852, -0.3812,\n",
      "        -1.2504, -0.2486, -1.0723, -0.3228, -0.2335,  0.8313, -0.6257],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "\n",
      " Models without bias:\n",
      "Model: Linear\n",
      "\n",
      "Parameter                 Shape                     Requires_grad\n",
      "----------------------------------------------------------------------\n",
      "weight                    [15, 10]                  True\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Total parameters: 150\n"
     ]
    }
   ],
   "source": [
    "# Linear layers \n",
    "dim_in = 10        # Input dimension\n",
    "dim_out = 15       # Output dimension\n",
    "\n",
    "linear_layer = torch.nn.Linear(dim_in, dim_out)\n",
    "\n",
    "# Showing the size of the (weight) matrix W_l and the bias vector b_l\n",
    "print_model_params(linear_layer)\n",
    "\n",
    "# Compute the results given a vector input x\n",
    "x = torch.randn(dim_in)\n",
    "print(linear_layer(x))\n",
    "\n",
    "# You can pass the option so that the linear layer does not have bias vector\n",
    "linear_layer_without_bias = torch.nn.Linear(dim_in, dim_out, bias = False)\n",
    "print(\"\\n\\n Models without bias:\")\n",
    "print_model_params(linear_layer_without_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac23d51",
   "metadata": {},
   "source": [
    "We can use $\\mathtt{torch.nn.Sequential}$ modules to group several linear and non-linear layers together. For non-linear functions such as ReLU, we have to use $\\mathtt{torch.nn.ReLU}$ instead of $\\mathtt{torch.relu}$. The former is a module while the latter is a function. By design, $\\mathtt{torch.nn.Sequential}$ takes a list of modules/classes, and not a function. Therefore, we cannot use $\\mathtt{torch.nn.ReLU}$ and $\\mathtt{torch.relu}$ interchangebly even if the two functions are semantically identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f4fb1729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Sequential\n",
      "\n",
      "Parameter                 Shape                     Requires_grad\n",
      "----------------------------------------------------------------------\n",
      "0.weight                  [20, 20]                  True\n",
      "0.bias                    [20]                      True\n",
      "2.weight                  [20, 20]                  True\n",
      "2.bias                    [20]                      True\n",
      "4.weight                  [1, 20]                   True\n",
      "4.bias                    [1]                       True\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Total parameters: 861\n",
      "Model: Sequential\n",
      "\n",
      "Parameter                 Shape                     Requires_grad\n",
      "----------------------------------------------------------------------\n",
      "linear1.weight            [20, 20]                  True\n",
      "linear1.bias              [20]                      True\n",
      "linear2.weight            [20, 20]                  True\n",
      "linear2.bias              [20]                      True\n",
      "linear3.weight            [1, 20]                   True\n",
      "linear3.bias              [1]                       True\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Total parameters: 861\n"
     ]
    }
   ],
   "source": [
    "dim = 20\n",
    "\n",
    "# Passing a series of modules and classes to torch.nn.Sequential to create a complex MLP\n",
    "neural_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(dim, dim),\n",
    "    torch.nn.ReLU(),                   # Remember to use torch.nn.ReLU, instead of torch.relu\n",
    "    torch.nn.Linear(dim, dim),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(dim, 1)\n",
    ")\n",
    "\n",
    "print_model_params(neural_network)\n",
    "\n",
    "# If you want to name the modules in your neural networks so that you gain meaningful access later, you can use OrderedDict\n",
    "from collections import OrderedDict\n",
    "\n",
    "neural_network_named = torch.nn.Sequential(\n",
    "    OrderedDict(\n",
    "        [\n",
    "            (\"linear1\", torch.nn.Linear(dim, dim)),\n",
    "            (\"relu1\", torch.nn.ReLU()),\n",
    "            (\"linear2\", torch.nn.Linear(dim, dim)),\n",
    "            (\"relu2\", torch.nn.ReLU()),\n",
    "            (\"linear3\", torch.nn.Linear(dim, 1))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print_model_params(neural_network_named)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b00547d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information of the first linear layers of my neural network\n",
      "Model: Linear\n",
      "\n",
      "Parameter                 Shape                     Requires_grad\n",
      "----------------------------------------------------------------------\n",
      "weight                    [20, 20]                  True\n",
      "bias                      [20]                      True\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Total parameters: 420\n",
      "Model: Linear\n",
      "\n",
      "Parameter                 Shape                     Requires_grad\n",
      "----------------------------------------------------------------------\n",
      "weight                    [20, 20]                  True\n",
      "bias                      [20]                      True\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Total parameters: 420\n"
     ]
    }
   ],
   "source": [
    "# If submodules are named (using OrderedDict), you have their access in the neural networks as follows\n",
    "print(\"Information of the first linear layers of my neural network\")\n",
    "print_model_params(neural_network_named.linear1)\n",
    "\n",
    "# You can do the same without OrderedDict, but you need to remember the ordered of your submodules\n",
    "print_model_params(neural_network[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a218b",
   "metadata": {},
   "source": [
    "The second way to build complex neural network is to build a new class inheriting $\\mathtt{nn.Module}$ (see [First lesson](1_Basics.ipynb) for a remind of classes in Python). This method allows a more flexible way to construct a neural network since they might not be just sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9b02eafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MLP\n",
      "\n",
      "Parameter                 Shape                     Requires_grad\n",
      "----------------------------------------------------------------------\n",
      "linear1.weight            [20, 20]                  True\n",
      "linear1.bias              [20]                      True\n",
      "linear2.weight            [20, 20]                  True\n",
      "linear2.bias              [20]                      True\n",
      "linear3.weight            [1, 20]                   True\n",
      "linear3.bias              [1]                       True\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Total parameters: 861\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    # Init function: usually where we define parameters\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(MLP, self).__init__() \n",
    "        \n",
    "        self.linear1 = nn.Linear(dim_in, dim_in)\n",
    "        self.linear2 = nn.Linear(dim_in, dim_in)\n",
    "        self.linear3 = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "        self.activation_func = F.relu\n",
    "\n",
    "    # Forward function: where we define how to compute an output, given an input\n",
    "    def forward(self, x):\n",
    "        x = self.activation_func(self.linear1(x))\n",
    "        x = self.activation_func(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x \n",
    "\n",
    "print_model_params(MLP(dim_in = dim, dim_out = 1))\n",
    "\n",
    "# We can customize our model with a greater flexibility\n",
    "# In the following, we will use linear 2 twice: after before linear 1 and after linear 1. This is impossible to do with Sequential\n",
    "\n",
    "class MLP_customized(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(MLP, self).__init__() \n",
    "        \n",
    "        self.linear1 = nn.Linear(dim_in, dim_in)\n",
    "        self.linear2 = nn.Linear(dim_in, dim_in)\n",
    "        self.linear3 = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "        self.activation_func = F.relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation_func(self.linear2(x))      # New line\n",
    "        x = self.activation_func(self.linear1(x))\n",
    "        x = self.activation_func(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ccd976",
   "metadata": {},
   "source": [
    "To learn the model, we need to define a loss function and perform optimization. As most algorithm in neural network training require gradient, we can use our previous method to compute gradient of parameters. We consider the simplest loss functions - the quadratic loss on a single sample:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(\\theta) = (f_\\theta(x) - y)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3cf0b49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.5559e-03,  1.3487e-01,  6.5151e-02, -5.1109e-02, -3.9494e-02,\n",
      "          8.7143e-02, -2.9409e-02,  1.0440e-01, -1.4997e-01, -1.0820e-01,\n",
      "         -4.9237e-02, -1.6397e-02, -9.2151e-02,  5.7651e-02, -1.1397e-01,\n",
      "         -1.1699e-01,  3.9388e-02, -3.2028e-02,  9.1441e-03,  7.2132e-02],\n",
      "        [ 8.7015e-04,  2.1124e-02,  1.0204e-02, -8.0045e-03, -6.1854e-03,\n",
      "          1.3648e-02, -4.6060e-03,  1.6351e-02, -2.3487e-02, -1.6946e-02,\n",
      "         -7.7115e-03, -2.5681e-03, -1.4432e-02,  9.0292e-03, -1.7850e-02,\n",
      "         -1.8323e-02,  6.1689e-03, -5.0161e-03,  1.4321e-03,  1.1297e-02],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [ 1.7989e-03,  4.3671e-02,  2.1095e-02, -1.6548e-02, -1.2788e-02,\n",
      "          2.8216e-02, -9.5223e-03,  3.3803e-02, -4.8558e-02, -3.5033e-02,\n",
      "         -1.5943e-02, -5.3092e-03, -2.9837e-02,  1.8667e-02, -3.6903e-02,\n",
      "         -3.7881e-02,  1.2753e-02, -1.0370e-02,  2.9607e-03,  2.3355e-02],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [ 8.0060e-03,  1.9435e-01,  9.3882e-02, -7.3647e-02, -5.6910e-02,\n",
      "          1.2557e-01, -4.2378e-02,  1.5044e-01, -2.1610e-01, -1.5591e-01,\n",
      "         -7.0951e-02, -2.3628e-02, -1.3279e-01,  8.3075e-02, -1.6423e-01,\n",
      "         -1.6858e-01,  5.6758e-02, -4.6152e-02,  1.3176e-02,  1.0394e-01],\n",
      "        [ 1.4701e-02,  3.5689e-01,  1.7240e-01, -1.3524e-01, -1.0450e-01,\n",
      "          2.3059e-01, -7.7819e-02,  2.7625e-01, -3.9683e-01, -2.8630e-01,\n",
      "         -1.3029e-01, -4.3388e-02, -2.4384e-01,  1.5255e-01, -3.0158e-01,\n",
      "         -3.0957e-01,  1.0423e-01, -8.4749e-02,  2.4196e-02,  1.9087e-01],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [-1.2062e-02, -2.9282e-01, -1.4145e-01,  1.1096e-01,  8.5745e-02,\n",
      "         -1.8920e-01,  6.3850e-02, -2.2666e-01,  3.2559e-01,  2.3491e-01,\n",
      "          1.0690e-01,  3.5599e-02,  2.0007e-01, -1.2517e-01,  2.4744e-01,\n",
      "          2.5400e-01, -8.5516e-02,  6.9535e-02, -1.9853e-02, -1.5660e-01],\n",
      "        [ 7.8364e-03,  1.9023e-01,  9.1894e-02, -7.2087e-02, -5.5705e-02,\n",
      "          1.2291e-01, -4.1481e-02,  1.4725e-01, -2.1152e-01, -1.5261e-01,\n",
      "         -6.9448e-02, -2.3127e-02, -1.2998e-01,  8.1315e-02, -1.6075e-01,\n",
      "         -1.6501e-01,  5.5556e-02, -4.5174e-02,  1.2897e-02,  1.0174e-01],\n",
      "        [ 1.0942e-02,  2.6562e-01,  1.2831e-01, -1.0065e-01, -7.7779e-02,\n",
      "          1.7162e-01, -5.7918e-02,  2.0560e-01, -2.9534e-01, -2.1308e-01,\n",
      "         -9.6968e-02, -3.2292e-02, -1.8148e-01,  1.1354e-01, -2.2445e-01,\n",
      "         -2.3040e-01,  7.7571e-02, -6.3075e-02,  1.8008e-02,  1.4206e-01],\n",
      "        [ 3.2348e-02,  7.8528e-01,  3.7934e-01, -2.9757e-01, -2.2995e-01,\n",
      "          5.0738e-01, -1.7123e-01,  6.0784e-01, -8.7316e-01, -6.2997e-01,\n",
      "         -2.8668e-01, -9.5469e-02, -5.3654e-01,  3.3567e-01, -6.6359e-01,\n",
      "         -6.8117e-01,  2.2933e-01, -1.8648e-01,  5.3240e-02,  4.1998e-01],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [ 2.0256e-02,  4.9174e-01,  2.3754e-01, -1.8634e-01, -1.4399e-01,\n",
      "          3.1771e-01, -1.0722e-01,  3.8063e-01, -5.4677e-01, -3.9448e-01,\n",
      "         -1.7952e-01, -5.9782e-02, -3.3597e-01,  2.1019e-01, -4.1553e-01,\n",
      "         -4.2654e-01,  1.4361e-01, -1.1677e-01,  3.3338e-02,  2.6299e-01],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [ 1.0959e-03,  2.6603e-02,  1.2851e-02, -1.0081e-02, -7.7901e-03,\n",
      "          1.7189e-02, -5.8009e-03,  2.0592e-02, -2.9581e-02, -2.1342e-02,\n",
      "         -9.7120e-03, -3.2343e-03, -1.8177e-02,  1.1372e-02, -2.2481e-02,\n",
      "         -2.3076e-02,  7.7692e-03, -6.3174e-03,  1.8036e-03,  1.4228e-02]])\n"
     ]
    }
   ],
   "source": [
    "# Learning samples\n",
    "x = torch.randn(dim)\n",
    "y = torch.randn(1)\n",
    "\n",
    "# Models\n",
    "neural_network = MLP(dim_in=dim, dim_out=1)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = (neural_network(x) - y) ** 2\n",
    "\n",
    "# Differentiation \n",
    "loss_fn.backward()\n",
    "\n",
    "# Get the gradient of the weight matrix of the first linear layer\n",
    "print(neural_network.linear1.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4646dd9",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa96438a",
   "metadata": {},
   "source": [
    "1. Compute the maximum eigenvalue of a symmetric matrix using power method in Pytorch: Given a semi-positive definite matrix $\\mathbf{A} \\in \\mathbb{R}^{d \\times d}$, one can compute the largest eigenvalue of $\\mathbf{A}$ as follows:\n",
    "- Randomize a vector $v \\in \\mathbb{R}^{d \\times d}$.\n",
    "- Repeatedly perform the following operation: $v \\gets \\frac{\\mathbf{A}v}{\\|\\mathbf{A}v\\|}$ for $k$ sufficiently large number of iterations.\n",
    "- Return $\\|v\\|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9ba6449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def your_function(A: torch.Tensor, k: int):\n",
    "    \"\"\"\n",
    "        Implement your power method here\n",
    "    \"\"\"\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "81881514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor(32.4726)\n"
     ]
    }
   ],
   "source": [
    "# Test the correctness of your method\n",
    "X = torch.randn(10,10)\n",
    "A = torch.matmul(X,X.T)    # A is semi-positive definite\n",
    "\n",
    "your_max_eigs = your_function(A, k = 20)\n",
    "true_max_eigs = max(torch.linalg.eigvals(A).real)\n",
    "\n",
    "print(your_max_eigs)\n",
    "print(true_max_eigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420a688",
   "metadata": {},
   "source": [
    "2. Compute the best rank-$r$ approximation of a given matrix: Given a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, we would like to find a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ whose rank is at most $r$ that minimizes $\\|\\mathbf{A} - \\mathbf{A}'\\|_F^2$. Mathematically, it is formulate as:\n",
    "\\begin{equation}\n",
    "\\text{Minimze} \\|\\mathbf{A} - \\mathbf{A}'\\|_F^2 \\qquad \\text{such that} \\qquad \\mathtt{rank}(\\mathbf{A}') \\leq r.\n",
    "\\end{equation}\n",
    "\n",
    "The analytic solution of this problem is given by:\n",
    "\n",
    "$$\\mathbf{A}' = \\mathbf{U}[:,r] \\mathtt{diag}(\\lambda_1, \\ldots, \\lambda_r) \\mathbf{V}[:,r]^\\top$$\n",
    "\n",
    "where $\\mathbf{U}, \\mathbf{V}$ are two orthogonal matrices in the SVD of $\\mathbf{A}$ and $\\lambda_1 \\geq \\ldots \\geq \\lambda_r$ are the $r$ largest singular values of $\\mathbf{A}$ (also given by the SVD, see Section 1.2).\n",
    "\n",
    "Write a function using Pytorch to find $\\mathbf{A}'$ given $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66182ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_rank_approximation(A: torch.Tensor, r: int):\n",
    "    \"\"\"Write your code here\"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78363594",
   "metadata": {},
   "source": [
    "3. Write a gradient descent for the neural network training problem: Consider optimizing the following function:\n",
    "\\begin{equation}\n",
    "f(\\mathbf{W}_1, \\mathbf{W}_2, b_1, b_2) = (y - \\mathbf{W}_2\\sigma(\\mathbf{W}_1 x + b_1) + b_2)^2,\n",
    "\\end{equation}\n",
    "where $\\sigma(x) = \\frac{1}{1 + \\exp(x)} $ applied in a coordinate-wise fashion (you can use $\\mathtt{torch.sigmoid}$). Once compute its gradient, use the gradient descent formula to update your parameters:\n",
    "\\begin{equation}\n",
    "    \\theta_{k+1} = \\theta_k - \\alpha \\nabla f(\\theta_k).\n",
    "\\end{equation}\n",
    "\n",
    "Attention: If you use $\\mathtt{backward()}$ to compute the gradient, make sure to reset them after every iteration by the command: $\\mathtt{your\\_params.grad.zero\\_()}$. Otherwise, the gradient will be accumulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c267c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5652a369",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
