{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17745c53",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td width=15%><img src=\"./img/UGA.png\"></img></td>\n",
    "<td><center><h1>Introduction to Python for Data Sciences</h1></center></td>\n",
    "<td width=15%><a href=\"https://tung-qle.github.io/\" style=\"font-size: 16px; font-weight: bold\">Quoc-Tung Le</a> </td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e8017c",
   "metadata": {},
   "source": [
    "# 1 - Pytorch: a Numpy library that can differentiate\n",
    "\n",
    "Pytorch is a Python library that are arguably the most popular for deep learning. It contains many similar functions that are implemented in Numpy (see the [second notebook](2_Numpy_and_co.ipynb)). Attention: many functions of these two libraries might have the same names, but their functionalities can be (entirely) different!\n",
    "\n",
    "As we will see in this tutorial, in comparison to Numpy, Pytorch provides an additional important feature: Automatic Differentiation (AD, sometimes shortened to autodiff). That means if we implement a function using Pytorch library, we can compute its gradient with respect to its parameters efficiently. The gradient will be then used in the optimization algorithm (see Optimization course for more details).\n",
    "\n",
    "The following code demonstrates the autodiff feature of Pytorch: in the following, we want to compute the gradient of the function:\n",
    "\n",
    "$$f(x) = \\frac{1}{2}x^\\top \\mathbf{A} x,$$\n",
    "\n",
    "which are given by: $\\nabla f(x) = \\frac{1}{2}(\\mathbf{A} + \\mathbf{A}^\\top)x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d56b8060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4024, -0.1146, -0.2406, -0.0727,  2.5713, -6.1343, -3.0981, -5.3953,\n",
      "        -3.2368,  0.7224, -1.7826,  2.3095,  4.7901,  7.4169,  4.8418, -0.2430,\n",
      "        -0.7804, -1.5638, -1.0425,  3.7074])\n",
      "None\n",
      "Two vectors are equal. All is good\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "dim = 20\n",
    "# Create a parameter x and a matrix A\n",
    "x = torch.randn(dim, requires_grad=True)\n",
    "A = torch.randn((dim, dim))\n",
    "\n",
    "# Compute the function f(x) and assign to the variable y\n",
    "y = 0.5 * torch.dot(x, torch.matmul(A, x))\n",
    "\n",
    "# Differentiating the function f by calling y.backward()\n",
    "y.backward()\n",
    "\n",
    "# Accessing the gradient of f with respect to x\n",
    "print(x.grad)\n",
    "print(A.grad)\n",
    "\n",
    "# Checking the calculation with the closed form gradient formula\n",
    "try:\n",
    "    torch.testing.assert_close(0.5 * torch.matmul(A + A.T,x), x.grad)\n",
    "    print(\"Two vectors are equal. All is good\")\n",
    "except:\n",
    "    print(\"Wrong calculation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8571b8bf",
   "metadata": {},
   "source": [
    "## 1.1 - How to create Pytorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdeb76a",
   "metadata": {},
   "source": [
    "There are many different methods to create a Pytorch tensor, either using Python list, numpy array or even randomization. They are shown in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba1fdb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [-3., -2., -1.]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize a tensor using Python list\n",
    "x = torch.Tensor([[1, 2, 3], [-3, -2, -1]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9b8c8",
   "metadata": {},
   "source": [
    "Unlike a Numpy array, a Pytorch tensor has many metadata field. The following code shows how to access the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad38f66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor info:\n",
      "  shape        : (2, 3)\n",
      "  size         : torch.Size([2, 3])\n",
      "  dtype        : torch.float32\n",
      "  device       : cpu\n",
      "  requires_grad: False\n",
      "  is_leaf      : True\n",
      "  grad_fn      : None\n",
      "  data         :\n",
      "tensor([[ 1.,  2.,  3.],\n",
      "        [-3., -2., -1.]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensor info:\")\n",
    "print(f\"  shape        : {tuple(x.shape)}\")\n",
    "print(f\"  size         : {x.size()}\")\n",
    "print(f\"  dtype        : {x.dtype}\")\n",
    "print(f\"  device       : {x.device}\")         # The output is either CPU or GPU, depending on the your implementation and hardware\n",
    "print(f\"  requires_grad: {x.requires_grad}\")  # If requires_grad = False, it is impossible to differentiate a function w.r.t. x. See the example with the quadratic function\n",
    "print(f\"  grad_fn      : {x.grad_fn}\")        # grad_fn\n",
    "print(f\"  is_leaf      : {x.is_leaf}\")        # is_leaf = True if and only if requires_grad = False (convention) or it is created by the user and not a result of some operations\n",
    "print(f\"  data         :\\n{x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee3abe1",
   "metadata": {},
   "source": [
    "## 1.2 - Operation on Pytorch tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e6511",
   "metadata": {},
   "source": [
    "## 1.3 - Linear algebra with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac98e10f",
   "metadata": {},
   "source": [
    "## 1.4 - Building functions using Pytorch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35da2ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "582bf51c",
   "metadata": {},
   "source": [
    "## 1.5 - Compute the gradient of a function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed4e15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4646dd9",
   "metadata": {},
   "source": [
    "## 1.6 - Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa96438a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
